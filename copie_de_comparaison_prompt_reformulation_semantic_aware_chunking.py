# -*- coding: utf-8 -*-
"""Copie de Comparaison prompt reformulation - Semantic-aware chunking

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18d2fZjhr6ML3dHBwMWEwjeoiezNjYVNp
"""

!pip install PyPDF2

pip install py2neo

# Import libraries
import re
import torch
import PyPDF2
import spacy
import unicodedata
import networkx as nx
import matplotlib.pyplot as plt
from google.colab import files
from openai import OpenAI
from sentence_transformers import SentenceTransformer, util

# Load the spaCy model and sentence-transformer model
nlp = spacy.load("en_core_web_sm")
embedder = SentenceTransformer('all-MiniLM-L6-v2')

########################################
# PART 1: PDF Loading and Text Extraction
########################################

print("Please upload your PhD thesis PDF file:")
uploaded = files.upload()
pdf_filename = list(uploaded.keys())[0]
print("Processing:", pdf_filename)

text = ""
with open(pdf_filename, 'rb') as pdf_file:
    pdf_reader = PyPDF2.PdfReader(pdf_file)
    for page in pdf_reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text + "\n"

print("\nExtracted text snippet (first 500 characters):")
print(text[:500])

########################################
# PART 2: Triplet Extraction Function
########################################

def extract_triplets(sentence):
    doc = nlp(sentence)
    triplets = []
    for sent in doc.sents:
        subject = None
        verb = None
        obj = None
        for token in sent:
            if token.dep_ in ("nsubj", "nsubjpass") and subject is None:
                subject = token.text
            if token.dep_ == "ROOT" and verb is None:
                verb = token.text
            if token.dep_ in ("dobj", "attr", "pobj") and obj is None:
                obj = token.text
        if subject and verb and obj:
            triplets.append((subject, verb, obj))
    return triplets

########################################
# PART 3: Build Thesis Knowledge Graph
########################################

sentences = re.split(r'(?<=[.!?]) +', text)
thesis_triplets = []
for s in sentences:
    triplets = extract_triplets(s)
    if triplets:
        thesis_triplets.extend(triplets)

print("\nExtracted triplets from thesis (first 10):")
print(thesis_triplets[:10])

G_thesis = nx.DiGraph()
for subj, verb, obj in thesis_triplets:
    G_thesis.add_node(subj)
    G_thesis.add_node(obj)
    G_thesis.add_edge(subj, obj, relation=verb)

########################################
# PART 4: Input Prompt and Reformulation using KAG + RAG
########################################

def format_triplet_natural(subj, verb, obj):
    if not verb.endswith('s'):
        verb += 's'
    return f"{subj} {verb} the {obj}"

def extract_acronyms(text):
    return list(set(re.findall(r'\b[A-Z]{2,10}\b', text)))

user_question = input("\nEnter your vanilla prompt: ")

# Identify acronyms from thesis and see which are mentioned in the prompt
all_acronyms = extract_acronyms(text)
mentioned_acronyms = [acronym for acronym in all_acronyms if acronym.lower() in user_question.lower()]


# RAG: Semantic Retrieval of Relevant Raw Text
# Semantic-aware chunking: group sentences into coherent chunks (~300 tokens max)
def semantic_chunk_text(text, max_tokens=300):
    doc = nlp(text)
    chunks, current_chunk = [], ""
    current_len = 0
    for sent in doc.sents:
        token_count = len(sent)
        if current_len + token_count > max_tokens:
            chunks.append(current_chunk.strip())
            current_chunk = str(sent)
            current_len = token_count
        else:
            current_chunk += " " + str(sent)
            current_len += token_count
    if current_chunk:
        chunks.append(current_chunk.strip())
    return chunks

text_chunks = semantic_chunk_text(text)

encoded_chunks = embedder.encode(text_chunks, convert_to_tensor=True)
encoded_question = embedder.encode(user_question, convert_to_tensor=True)
chunk_scores = util.pytorch_cos_sim(encoded_question, encoded_chunks)[0]
top_chunks = torch.topk(chunk_scores, k=3)
retrieved_texts = [text_chunks[i] for i in top_chunks[1]]
rag_context = "\n\n".join([f"\"{chunk.strip()}\"" for chunk in retrieved_texts])

# KAG: Triplet Selection
triplet_tuples = [(s, v, o) for (s, v, o) in thesis_triplets]
triplet_sentences = [f"{s} {v} {o}" for (s, v, o) in triplet_tuples]
encoded_triplets = embedder.encode(triplet_sentences, convert_to_tensor=True)
cos_scores = util.pytorch_cos_sim(encoded_question, encoded_triplets)[0]
top_k = min(5, len(cos_scores))
top_results = torch.topk(cos_scores, k=top_k)
selected_triplets = [triplet_tuples[idx] for idx in top_results[1]]
triplet_text = "\n".join([f"- The thesis states that {format_triplet_natural(*t)}." for t in selected_triplets])

# Final Reformulated Prompt (RAG + KAG Fusion)
rag_prompt = f"""You are an assistant answering based only on the user's research thesis.

Contextual evidence:
{rag_context}

Now, answer the question: {user_question}
"""
# Final Reformulated Prompt (RAG + KAG Fusion)
kag_prompt = f"""You are an assistant answering based only on the user's research thesis.

Structured facts extracted from the thesis:
{triplet_text}


Now, answer the question: {user_question}
"""
# Final Reformulated Prompt (RAG + KAG Fusion)
kag_rag_prompt = f"""You are an assistant answering based only on the user's research thesis.

Structured facts extracted from the thesis:
{triplet_text}

Contextual evidence:
{rag_context}

Now, answer the question: {user_question}
"""

def safe_print(text):
    print(unicodedata.normalize("NFKD", text).encode("utf-8", errors="ignore").decode("utf-8", errors="ignore"))


print("\nüìú Reformulated Prompt (RAG):\n")
safe_print(rag_prompt)

print("\nüìú Reformulated Prompt (KAG):\n")
safe_print(kag_prompt)

print("\nüìú Reformulated Prompt (KAG + RAG):\n")
safe_print(kag_rag_prompt)


# Initialize the NVIDIA OpenAI-compatible client only once
client = OpenAI(
    base_url="https://integrate.api.nvidia.com/v1",
    api_key="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"
)

# === Model Call 0: Vanilla Prompt ===
print("\n‚è≥ Querying model with VANILLA prompt (no RAG, no KAG)...")
response_vanilla = client.chat.completions.create(
    model="meta/llama3-8b-instruct",
    messages=[{"role": "user", "content": user_question}],
    temperature=0.6,
    top_p=0.95,
    max_tokens=300
)
output_text_vanilla = response_vanilla.choices[0].message.content
print("\n‚úÖ Model output (based on VANILLA prompt):")
print(output_text_vanilla)


# === Model Call 1: RAG ===
print("\n‚è≥ Querying model with RAG-enhanced prompt...")
response_rag = client.chat.completions.create(
    model="meta/llama3-8b-instruct",
    messages=[{"role": "user", "content": rag_prompt}],
    temperature=0.6,
    top_p=0.95,
    max_tokens=300
)
output_text_rag = response_rag.choices[0].message.content
print("\n‚úÖ Model output (based on RAG):")
print(output_text_rag)

# === Model Call 2: KAG ===
print("\n‚è≥ Querying model with KAG-enhanced prompt...")
response_kag = client.chat.completions.create(
    model="meta/llama3-8b-instruct",
    messages=[{"role": "user", "content": kag_prompt}],
    temperature=0.6,
    top_p=0.95,
    max_tokens=300
)
output_text_kag = response_kag.choices[0].message.content
print("\n‚úÖ Model output (based on KAG):")
print(output_text_kag)

# === Model Call 3: KAG + RAG ===
print("\n‚è≥ Querying model with KAG + RAG-enhanced prompt...")
response_kag_rag = client.chat.completions.create(
    model="meta/llama3-8b-instruct",
    messages=[{"role": "user", "content": kag_rag_prompt}],
    temperature=0.6,
    top_p=0.95,
    max_tokens=300
)
output_text_kag_rag = response_kag_rag.choices[0].message.content
print("\n‚úÖ Model output (based on KAG + RAG):")
print(output_text_kag_rag)


########################################
# PART 5: Graph Extraction and Visualization (Multi-Prompt)
########################################

def clean_for_kg(text):
    return text.replace("The thesis states that", "").replace("‚Äì", "-")

def process_text_to_graph(text_str):
    triplets = []
    for s in re.split(r'(?<=[.!?]) +', text_str):
        triplets.extend(extract_triplets(clean_for_kg(s)))
    G = nx.DiGraph()
    for subj, verb, obj in triplets:
        G.add_node(subj)
        G.add_node(obj)
        G.add_edge(subj, obj, relation=verb)
    return G, triplets

# Build graphs for each variant
graphs_and_triplets = {
    "Vanilla": process_text_to_graph(output_text_vanilla),
    "RAG": process_text_to_graph(output_text_rag),
    "KAG": process_text_to_graph(output_text_kag),
    "RAG+KAG": process_text_to_graph(output_text_kag_rag),
}

# Display triplets
for label, (G_output, triplets) in graphs_and_triplets.items():
    print(f"\nüß† Extracted triplets from {label} output:")
    print(triplets)

# Prompt graphs (used for structure evaluation)
G_prompt_vanilla, _ = process_text_to_graph(user_question)
G_prompt_rag, _ = process_text_to_graph(rag_prompt)
G_prompt_kag, _ = process_text_to_graph(kag_prompt)
G_prompt_kag_rag, _ = process_text_to_graph(kag_rag_prompt)

prompt_graphs = {
    "Vanilla": G_prompt_vanilla,
    "RAG": G_prompt_rag,
    "KAG": G_prompt_kag,
    "RAG+KAG": G_prompt_kag_rag
}

# Acronym auditing
def audit_graph_for_acronyms(G, acronyms, label):
    missing = [term for term in acronyms if term not in G.nodes()]
    if missing:
        print(f"\n‚ö†Ô∏è {label} KG is missing key acronym(s): {', '.join(missing)}")
    else:
        print(f"\n‚úÖ All key acronyms from prompt are present in {label} KG.")

for variant in ["RAG", "KAG", "RAG+KAG"]:
    audit_graph_for_acronyms(prompt_graphs[variant], mentioned_acronyms, f"{variant} Prompt")
    audit_graph_for_acronyms(graphs_and_triplets[variant][0], mentioned_acronyms, f"{variant} Output")

########################################
# PART 6: Graph Comparison Heuristics (Multi-Prompt)
########################################

def visualize_graph(G, title):
    plt.figure(figsize=(8, 8))
    pos = nx.spring_layout(G, k=0.5)
    nx.draw(G, pos, with_labels=True, node_size=1500, node_color='lightblue', font_size=10, font_weight='bold')
    edge_labels = nx.get_edge_attributes(G, 'relation')
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red')
    plt.title(title)
    plt.show()

# Visualize thesis graph once
visualize_graph(G_thesis, "Knowledge Graph from Thesis PDF")

# Visualize each variant
for variant in ["Vanilla", "RAG", "KAG", "RAG+KAG"]:
    visualize_graph(prompt_graphs[variant], f"Knowledge Graph from {variant} Prompt")
    visualize_graph(graphs_and_triplets[variant][0], f"Knowledge Graph from {variant} Output")

# Graph similarity function
def graph_similarity(G1, G2):
    nodes1 = set(G1.nodes())
    nodes2 = set(G2.nodes())
    jaccard_nodes = len(nodes1 & nodes2) / len(nodes1 | nodes2) if nodes1 | nodes2 else 0

    edges1 = set((u, v, d['relation']) for u, v, d in G1.edges(data=True))
    edges2 = set((u, v, d['relation']) for u, v, d in G2.edges(data=True))
    jaccard_edges = len(edges1 & edges2) / len(edges1 | edges2) if edges1 | edges2 else 0

    return jaccard_nodes, jaccard_edges

# Compute similarities and collect metrics
graph_metrics = []
THRESHOLD = 0.3

for variant in ["Vanilla", "RAG", "KAG", "RAG+KAG"]:
    G_prompt = prompt_graphs[variant]
    G_output = graphs_and_triplets[variant][0]

    sim_prompt_thesis = graph_similarity(G_prompt, G_thesis)
    sim_output_thesis = graph_similarity(G_output, G_thesis)
    sim_prompt_output = graph_similarity(G_prompt, G_output)

    print(f"\nüìä Graph Similarity Metrics for {variant}:")
    print(f"- Prompt vs. Thesis (Nodes, Edges): {sim_prompt_thesis}")
    print(f"- Output vs. Thesis (Nodes, Edges): {sim_output_thesis}")
    print(f"- Prompt vs. Output (Nodes, Edges): {sim_prompt_output}")

    if sim_prompt_thesis[0] < THRESHOLD:
        print(f"[Heuristic Alert] {variant} Prompt graph has low similarity with thesis. Consider improving alignment.")
    else:
        print(f"{variant} Prompt graph is well-aligned with thesis.")

    if sim_output_thesis[0] < THRESHOLD:
        print(f"[Heuristic Alert] {variant} Output graph weakly grounded in thesis. Possible hallucination.")
    else:
        print(f"{variant} Output is grounded in thesis knowledge.")

    if sim_prompt_output[0] < THRESHOLD:
        print(f"[Heuristic Alert] {variant} Output doesn't follow prompt structure. Prompt might need refinement.")
    else:
        print(f"{variant} Prompt and Output are well aligned.")

    graph_metrics.append({
        "Variant": variant,
        "Prompt vs Thesis (Nodes)": sim_prompt_thesis[0],
        "Prompt vs Thesis (Edges)": sim_prompt_thesis[1],
        "Output vs Thesis (Nodes)": sim_output_thesis[0],
        "Output vs Thesis (Edges)": sim_output_thesis[1],
        "Prompt vs Output (Nodes)": sim_prompt_output[0],
        "Prompt vs Output (Edges)": sim_prompt_output[1]
    })

# Create summary DataFrame
import pandas as pd
df_graph_metrics = pd.DataFrame(graph_metrics)
print("\nüìã Summary of Graph Similarity Metrics:")
display(df_graph_metrics)


########################################
# PART 7: Semantic Similarity Evaluation (Multi-Variant)
########################################

def compute_asymmetric_similarity(a_texts, b_texts, model):
    a_embeddings = model.encode(a_texts, convert_to_tensor=True)
    b_embeddings = model.encode(b_texts, convert_to_tensor=True)
    sim_matrix = util.pytorch_cos_sim(a_embeddings, b_embeddings)
    avg_a_to_b = torch.max(sim_matrix, dim=1).values.mean().item()
    avg_b_to_a = torch.max(sim_matrix, dim=0).values.mean().item()
    return avg_a_to_b, avg_b_to_a

semantic_metrics = []

for variant, output_text in {
    "Vanilla": output_text_vanilla,
    "RAG": output_text_rag,
    "KAG": output_text_kag,
    "RAG+KAG": output_text_kag_rag
}.items():
    prompt_text = {
        "Vanilla": user_question,
        "RAG": rag_prompt,
        "KAG": kag_prompt,
        "RAG+KAG": kag_rag_prompt
    }[variant]

    sim_prompt_context, sim_context_prompt = compute_asymmetric_similarity([prompt_text], retrieved_texts, embedder)
    sim_output_context, sim_context_output = compute_asymmetric_similarity([output_text], retrieved_texts, embedder)
    sim_output_prompt, sim_prompt_output = compute_asymmetric_similarity([output_text], [prompt_text], embedder)

    # Acronym coverage
    G_prompt = prompt_graphs[variant]
    G_output = graphs_and_triplets[variant][0]
    covered_in_prompt = [a for a in mentioned_acronyms if a in G_prompt.nodes()]
    covered_in_output = [a for a in mentioned_acronyms if a in G_output.nodes()]
    acronym_coverage_prompt = len(covered_in_prompt) / len(mentioned_acronyms) if mentioned_acronyms else 1.0
    acronym_coverage_output = len(covered_in_output) / len(mentioned_acronyms) if mentioned_acronyms else 1.0

    # Store metrics
    semantic_metrics.append({
        "Variant": variant,
        "Prompt ‚Üí Context": sim_prompt_context,
        "Context ‚Üí Prompt": sim_context_prompt,
        "Output ‚Üí Context": sim_output_context,
        "Context ‚Üí Output": sim_context_output,
        "Output ‚Üí Prompt": sim_output_prompt,
        "Prompt ‚Üí Output": sim_prompt_output,
        "Acronym Coverage (Prompt)": acronym_coverage_prompt,
        "Acronym Coverage (Output)": acronym_coverage_output
    })

# Display full metric table
df_semantic_metrics = pd.DataFrame(semantic_metrics)
print("\nüìã Semantic Similarity + Acronym Coverage Summary:")
display(df_semantic_metrics)

df_all_metrics = pd.merge(df_semantic_metrics, df_graph_metrics, on="Variant", how="inner")
print("\nüìä FULL METRICS TABLE:")
display(df_all_metrics)